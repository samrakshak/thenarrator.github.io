<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment) | The Narrator</title><meta name=keywords content><meta name=description content="AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment) This article is describing the thing you need to aware when using ALB Ingress Controller (AWS Load Balancer Controller) to do deployment in your kubernetes appication and prevent 502 errors when there is rolling updates.
What is AWS Load Balancer Controler Kubernetes doesn’t involve the Application Load Balancer (ALB) deployment in the native implementation for using Kubernetes service object with type=LoadBalancer."><meta name=author content="Samrakshak karki"><link rel=canonical href=https://samrakshak.github.io/thenarrator.github.io/post/aws-load-balancer-controller-issue/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/thenarrator.github.io/assets/css/stylesheet.8b523f1730c922e314350296d83fd666efa16519ca136320a93df674d00b6325.css integrity="sha256-i1I/FzDJIuMUNQKW2D/WZu+hZRnKE2MgqT32dNALYyU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/thenarrator.github.io/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://samrakshak.github.io/thenarrator.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://samrakshak.github.io/thenarrator.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://samrakshak.github.io/thenarrator.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://samrakshak.github.io/thenarrator.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://samrakshak.github.io/thenarrator.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment)"><meta property="og:description" content="AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment) This article is describing the thing you need to aware when using ALB Ingress Controller (AWS Load Balancer Controller) to do deployment in your kubernetes appication and prevent 502 errors when there is rolling updates.
What is AWS Load Balancer Controler Kubernetes doesn’t involve the Application Load Balancer (ALB) deployment in the native implementation for using Kubernetes service object with type=LoadBalancer."><meta property="og:type" content="article"><meta property="og:url" content="https://samrakshak.github.io/thenarrator.github.io/post/aws-load-balancer-controller-issue/"><meta property="og:image" content="https://samrakshak.github.io/thenarrator.github.io/static/images"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-07-17T22:53:49+05:45"><meta property="article:modified_time" content="2022-07-17T22:53:49+05:45"><meta property="og:site_name" content="TheNarrator"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://samrakshak.github.io/thenarrator.github.io/static/images"><meta name=twitter:title content="AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment)"><meta name=twitter:description content="AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment) This article is describing the thing you need to aware when using ALB Ingress Controller (AWS Load Balancer Controller) to do deployment in your kubernetes appication and prevent 502 errors when there is rolling updates.
What is AWS Load Balancer Controler Kubernetes doesn’t involve the Application Load Balancer (ALB) deployment in the native implementation for using Kubernetes service object with type=LoadBalancer."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://samrakshak.github.io/thenarrator.github.io/post/"},{"@type":"ListItem","position":2,"name":"AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment)","item":"https://samrakshak.github.io/thenarrator.github.io/post/aws-load-balancer-controller-issue/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment)","name":"AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment)","description":"AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment) This article is describing the thing you need to aware when using ALB Ingress Controller (AWS Load Balancer Controller) to do deployment in your kubernetes appication and prevent 502 errors when there is rolling updates.\nWhat is AWS Load Balancer Controler Kubernetes doesn’t involve the Application Load Balancer (ALB) deployment in the native implementation for using Kubernetes service object with type=LoadBalancer.","keywords":[],"articleBody":"AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment) This article is describing the thing you need to aware when using ALB Ingress Controller (AWS Load Balancer Controller) to do deployment in your kubernetes appication and prevent 502 errors when there is rolling updates.\nWhat is AWS Load Balancer Controler Kubernetes doesn’t involve the Application Load Balancer (ALB) deployment in the native implementation for using Kubernetes service object with type=LoadBalancer. Therefore, if you need to expose your container service wih Appication Load Balancer (ALB) on EKS which should be the prefered way to expose your servies, it is recommneded to integrate with AWS Load Balancer Controller . Guide for instalation: AWS ALB Installaion\nHow does it work source: aws-loadbalancer-controller\nThis section describes each step (circle) above. This example demonstrates satisfying 1 ingress resource.\n[1]: The controller watches for ingress events from the API server. When it finds ingress resources that satisfy its requirements, it begins the creation of AWS resources.\n[2]: An ALB (ELBv2) is created in AWS for the new ingress resource. This ALB can be internet-facing or internal. You can also specify the subnets it’s created in using annotations.\n[3]: Target Groups are created in AWS for each unique Kubernetes service described in the ingress resource.\n[4]: Lilsteners are created for every port detailed in your ingress resource annotations. When no port is specified, sensible defaults (80 or 443) are used. Certificates may also be attached via annotations.\n[5]: Rules are created for each path specified in your ingress resource. This ensures traffic to a specific path is routed to the correct Kubernetes Service.\nAlong with the above, the controller also…\ndeletes AWS components when ingress resources are removed from k8s. modifies AWS components when ingress resources change in k8s. assembles a list of existing ingress-related AWS components on start-up, allowing you to recover if the controller were to be restarted. Note: [AWS Load Balancer Controller](\u003e GitHub project - kubernetes-sigs/aws-alb-ingress-controller)\nHow to deploy Kubernetes with AWS Load Balancer Controller? Using Application Load Balancer as example, when running the controller, AWS Load Balancer Controller will be deployed as a Pod running on your worker node while continously monitor/watch your cluster state. Once there have any request for Ingress object creation, AWS Load Balancer Controller will help you to manage and create Application Load Balancer resource. install aws-alb-controller\nThe deployment basically will run a copy of ALB Ingress Controller (pod/alb-ingress-controller-xxxxxxxx-xxxxx) in kube-system:\nNAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/alb-ingress-controller-5fd8d5d894-8kf7z 1/1 Running 0 28s NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/alb-ingress-controller 1/1 1 1 3m48s In addition, the service can be deployed as Ingress Object. For example, if you tried to deploy the simple 2048 application:\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-ingress.yaml The file 2048-ingress.yaml is mentioning the annotations, spec in format that supported by ALB Ingress Controller can recognize (Before Kubernetes 1.18):\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: \"2048-ingress\" namespace: \"2048-game\" annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing labels: app: 2048-ingress spec: rules: - http: paths: - path: /* backend: serviceName: \"service-2048\" servicePort: 80 The ingress object will construct ELB Listeners according rules and forward the connection to the corresponding backend(serviceName), which match the group of service service-2048, any traffic match the rule /* will be routed to the group of selected Pods. In this case, Pods are exposed on the worker node based on type=NodePort:\nHere is the definition of this Kubernetes service:\napiVersion: v1 kind: Service metadata: name: \"service-2048\" namespace: \"2048-game\" spec: ports: - port: 80 targetPort: 80 protocol: TCP type: NodePort selector: app: \"2048\" So … what’s the problem? Zero downtime deployment is always a big challenge for DevOps/Operation team when running any kind of business. When you try to apply the controller as a solution to expose your service, it has a couple of things need to take care due to the behavior of Kubernetes, ALB and AWS Load Balancer Controller. To achieve zero downtime, you need to consider many perspectives, some new challenges will also popup when you would like to roll out the new deployment for your Pods with AWS Load Balancer Controller.\nLet’s use the 2048 game as example to describe the scenario when you are trying to roll out a new version of your container application. In my environment, I have:\nA Kubernetes service service/service-2048 using NodePort to expose the service The deployment also have 5 copy of Pods for 2048 game, which is my backend application waiting for connections forwarding by Application Load Balancer (ALB) NAMESPACE NAME READY STATUS RESTARTS AGE 2048-game pod/2048-deployment-58fb66554b-2f748 1/1 Running 0 53s 2048-game pod/2048-deployment-58fb66554b-4hz5q 1/1 Running 0 53s 2048-game pod/2048-deployment-58fb66554b-jdfps 1/1 Running 0 53s 2048-game pod/2048-deployment-58fb66554b-rlpqm 1/1 Running 0 53s 2048-game pod/2048-deployment-58fb66554b-s492n 1/1 Running 0 53s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 2048-game service/service-2048 NodePort 10.100.53.119 80:30337/TCP 52s NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE 2048-game deployment.apps/2048-deployment 5/5 5 5 53s And for sure, once the controller correctly set up and provision the ELB resource, the full domain of ELB also will be recorded to the Ingress object:\n$ kubectl get ingress -n 2048-game NAME HOSTS ADDRESS PORTS AGE 2048-ingress * xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com 80 11m I can use the DNS name as endpoint to visit my container service:\n$ curl -s xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com | head \u003c!DOCTYPE html\u003e 2048 ... This application can be any kind of critical service you are running. As a SRE (Site Reliability Engineer), the goal and your duty is: we always try to ensure the service can run properly without any issue and no interruption. But, no one can one hundred percent guarantees the service can run properly if any changes applied, because system usually has its limitation and trade-off. Any service downtime can lead anyone of stakeholders(users, operation team or leadership) unhappy.\nHowever, the question is that can we better to address these problem once we know the limitation and its behavior? In the following section I will try to walk through more realistic logic and phenomena by using the 2048 game as my sample service.\nI am going to use a simple loop trick to continously access my service via the endpoint [xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com](http://xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com/) to demonstrate a scenario: This is a popular web service and we always have customer need to access it, we basically have zero tolerance for any service downtime , as below:\n$ while true;do ./request-my-service.sh; sleep 0.1; done HTTPCode=200_TotalTime=0.010038 HTTPCode=200_TotalTime=0.012131 HTTPCode=200_TotalTime=0.005366 HTTPCode=200_TotalTime=0.010119 HTTPCode=200_TotalTime=0.012066 HTTPCode=200_TotalTime=0.005451 HTTPCode=200_TotalTime=0.010006 HTTPCode=200_TotalTime=0.012084 HTTPCode=200_TotalTime=0.005598 HTTPCode=200_TotalTime=0.010086 HTTPCode=200_TotalTime=0.012162 HTTPCode=200_TotalTime=0.005278 HTTPCode=200_TotalTime=0.010326 HTTPCode=200_TotalTime=0.012193 HTTPCode=200_TotalTime=0.005347 ... Meanwhile, I am using RollingUpdate strategy in my Kubernetes deployment strategy with maxUnavailable=25%, which means, when Kubernetes need to update or patch(Like update the image or environment variables), the maximum number of unavailable Pods cannot exceed over 25% as well as it ensures that at least 75% of the desired number of Pods are up (only replace 1-2 Pods if I have 5 copies at the same time):\napiVersion: apps/v1 kind: Deployment metadata: name: 2048-deployment namespace: 2048-game spec: ... selector: matchLabels: app: \"2048\" ... strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate Scenario: Rolling the new container image to existing container application with potential service downtime When rolling the new version of my container application (for example, I update my deployment by replacing the container image with the new image nginx), it potentially can have a period of time that can return HTTP Status Code 502 error in my few hits.\nIf you are specifying the controller to use instance mode to register targets(Pods) to your ELB Target Group, it will use worker nodes’ instance ID and expose your service in that ELB target group with Kubernetes NodePort. In this case, the traffic will follow the Kubernetes networking design to do second tier of transmission according to externalTrafficPolicy defined in the Kubernetes Service object (No matter using externalTrafficPolicy=Cluster or externalTrafficPolicy=Local).\nDue to the controller only care about to register Worker Node to the ELB target group, so if the scenario doesn’t involve the worker node replacement, the case basically have miniumun even no downtime(expect that it is rare to have downtime if the Kubernetes can perfectly handle the traffic forwarding); however, this is not how real world operate, few seconds downtime still can happen potentially due to the workflow below:\nThis is the general workflow when the client reach out to the service endpoint (ELB) and how was traffic goes\nClient ----\u003e ELB ----\u003e Worker Node (iptables) / In this step it might be forwarded to other Worker Node ----\u003e Pod So, in these cases, you can see the downtime:\n(1) The client established the connection with ELB, ELB is trying to forward the request to the backend (the Worker Node), but the Worker Node is not ready to serve the Pod. (2) Follow the iptables rules, the traffic be forwarded to the Pod just terminated due to RollingUpdate (Or the Pod just got the in-flight reqeust but immediately need to be terminated, the Pod flip to Terminating state. It haven’t response back yet, caused the ELB doesn’t get the response from Pod.) (3) ELB established connection with the Worker Node-1, once the packet enter into the Worker Node-1, it follows the iptables then forward it to the Pod running on Worker Node-2 (jump out the current worker node), however, the Worker Node-2 just got terminated due to auto scaling strategy or any replacement due to upgrade, caused the connection lost. Let’s say if you try to remove the encapsulation layer of the Kubernetes networking design and make thing more easier based on the AWS supported CNI Plugin (Only rely on the ELB to forward the traffic to the Pod directly by using IP mode with annotation setting [alb.ingress.kubernetes.io/target-type](http://alb.ingress.kubernetes.io/target-type): ip in my Ingress object), you can see the downtime more obvious when Pod doing RollingUpdate. That’s because not only the problem we mentioned the issues in case (1)/(2)/(3), but also there has different topic on the behavior of the controller need to be covered if the question comes to zero downtime deployment:\nHere is an example by using IP mode ([alb.ingress.kubernetes.io/target-type](http://alb.ingress.kubernetes.io/target-type): ip) as registration type to route traffic directly to the Pod IP\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: namespace: game-2048 name: ingress-2048 annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip spec: ingressClassName: alb rules: - http: paths: - path: / pathType: Prefix backend: service: name: service-2048 port: number: 80 An example when using IP mode in AWS Load Balancer Controller - Can see my Pods all are registering with Pod owns IP address\nAgain follow the issue we mentioned (1) (2) (3), when doing the rolling update (I was replacing the image again in IP mode), similar problem can be observed. Potentially, you can have 10-15 seconds even longer downtime can be noticed if you are doing the same lab:\nThe HTTP 502 Error response from ELB during the rolling update deployment (IP mode)\nWhen Kubernetes is rolling the deployment, in the target group, you will see AWS Load Balancer Controller was issuing old targets draining process(Old Pods) in the meantime\nOld targets were going to be draining state in target group\nHowever, you still can see HTTP 502/504 errors exceed 3-10 seconds for a single requset\nHTTPCode=200_TotalTime=0.005413 2048 HTTPCode=200_TotalTime=0.009980 502 Bad Gateway HTTPCode=502_TotalTime=3.076954 2048 HTTPCode=200_TotalTime=0.005700 2048 HTTPCode=200_TotalTime=0.010019 502 Bad Gateway HTTPCode=502_TotalTime=3.081601 2048 HTTPCode=200_TotalTime=0.005527 502 Bad Gateway HTTPCode=502_TotalTime=3.070947 502 Bad Gateway HTTPCode=502_TotalTime=3.187812 504 Gateway Time-out HTTPCode=504_TotalTime=10.006324 Welcome to nginx! HTTPCode=200_TotalTime=0.011838 Welcome to nginx! The issue and the workflow of the AWS Load Balancer Controller Let’s use this scenario as it is a edge problem we need to consider for most use case. The issue generally is bringing out the core topic we want to address and giving a good entry point to dive deep into the workflow between the Kubernetes, AWS Load Balancer Controller and the ELB, which can lead HTTP status code 502/503(5xx) erros during deployment when having Pod termination.\nBefore diving into it, we need to know when a pod is being replaced, AWS Load Balancer Controller will register the new pod in the target group and removes the old Pods. However, at the same time:\nFor the the new Pods, the target is in initial state, until it pass the defined health check threshold (ALB health check) For the old Pods is remaining as draining state, until it completes draining action for the in-flight connection, or reaching out the Deregistration delay defined in the target group. Which result in the service to be unavailable and return HTTP 502.\nTo better understand that, I made the following diagrams. It might be helpful to you understanding the workflow:\nHow to resolve the issue and meet zero-downtime? Factor-1: Pod Readiness Gates Since version v1.1.6, AWS Load Balancer Controller (ALB Ingress Controller) introduced Pod readiness gates. This feature can monitor the rolling deployment state and trigger the deployment pause due to any unexpected issue(such as: getting timeout error for AWS APIs), which guarantees you always have Pods in the Target Group even having issue on calling ELB APIs when doing rolling update.\nAWS Load Balancer Controller (After v2.x) For now, if you are using controller later than v2, the readiness gate configuration can be automatically injected to the pod spec by defining the label [elbv2.k8s.aws/pod-readiness-gate-inject](http://elbv2.k8s.aws/pod-readiness-gate-inject): enabled to your Kubernetes namespace.\n$ kubectl create namespace readiness namespace/readiness created $ kubectl label namespace readiness elbv2.k8s.aws/pod-readiness-gate-inject=enabled namespace/readiness labeled $ kubectl describe namespace readiness Name: readiness Labels: elbv2.k8s.aws/pod-readiness-gate-inject=enabled Annotations: Status: Active So defining legacy fields readinessGates and conditionType are not required if you are using controller later than v2.0. If you have a pod spec with legacy readiness gate configuration, ensure you label the namespace and create the Service/Ingress objects before applying the pod/deployment manifest. The controller will remove all legacy readiness-gate configuration and add new ones during pod creation.\nFactor-2: Graceful shutdown your applications For existing connections(As mentioned in the workflow-4), the case is involving the gracefully shutdown/termination handling in Kubernetes. Therefore, it is requires to use the method provided by Kubernetes.\nYou can use Pod Lifecycle with preStop hook and make some pause(like using sleep command) for Pod termination. This trick ensures ALB can have some time to completely remove old targets on Target Group (It is recommended to adjust longer based on your Deregistration delay):\nlifecycle: preStop: exec: command: [\"/bin/sh\", \"-c\", \"sleep 40\"] terminationGracePeriodSeconds: 70 Note: If a container has a preStop hook configured, that runs before the container enters the Terminated state. Also, if the preStop hook needs longer to complete than the default grace period allows, you must modify terminationGracePeriodSeconds to suit this.\nAn example to achieve zero downtime when doing rolling update after applying methods above First apply the label to the namespace so the controller can automatically inject the readiness gate:\napiVersion: v1 kind: Namespace metadata: name: 2048-game labels: elbv2.k8s.aws/pod-readiness-gate-inject: enabled apiVersion: apps/v1 kind: Deployment metadata: name: \"2048-deployment\" namespace: \"2048-game\" spec: selector: matchLabels: app: \"2048\" replicas: 5 template: metadata: labels: app: \"2048\" spec: # This would be optional if you are using controller after v2.x readinessGates: - conditionType: target-health.alb.ingress.k8s.aws/2048-ingress_service-2048_80 terminationGracePeriodSeconds: 70 containers: - image: alexwhen/docker-2048 imagePullPolicy: Always name: \"2048\" ports: - containerPort: 80 lifecycle: preStop: exec: command: [\"/bin/sh\", \"-c\", \"sleep 40\"] Here is an example after following the practice I was getting a try. The deployment will apply the feature and can see the status of the readiness gates:\n$ kubectl get pods -n 2048-game -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 2048-deployment-99b6fb474-c97ht 1/1 Running 0 78s 192.168.14.209 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal 1/1 2048-deployment-99b6fb474-dcxfs 1/1 Running 0 78s 192.168.31.47 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal 1/1 2048-deployment-99b6fb474-kvhhh 1/1 Running 0 54s 192.168.29.6 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal 1/1 2048-deployment-99b6fb474-vhjbg 1/1 Running 0 54s 192.168.18.161 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal 1/1 2048-deployment-99b6fb474-xfd5q 1/1 Running 0 78s 192.168.16.183 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal 1/1 Once rolling the new version of the container image, the deployment goes smoothly and prevent the downtime issue as mentioned in previous paragraphs:\nZero downtime with AWS Load Balancer Controller - Can see the targets are gracefully replaced when the Kubernetes is doing rolling update\nIn my scenario, the Kubernetes need to take at least 40 seconds termination period for single Pod, so the old targets are gradually moved out instead of remove all of them at once within few seconds, until entire target group only exists new targets.\nTherefore, you probably also need to notice the Deregistration delay defined in your ELB Target Group, which can be updated through the annotation:\nalb.ingress.kubernetes.io/target-group-attributes: deregistration_delay.timeout_seconds=30 In this case, it is recommended to be less than 40 seconds so ELB can drain your old targets before the Pod completely shutdown.\nWith the configuration, client can get normal responses from old Pods/existing connection during the deployment:\nHTTPCode=200_TotalTime=0.012028 2048 HTTPCode=200_TotalTime=0.005383 2048 HTTPCode=200_TotalTime=0.010174 2048 HTTPCode=200_TotalTime=0.012233 Welcome to nginx! HTTPCode=200_TotalTime=0.007116 2048 HTTPCode=200_TotalTime=0.010090 2048 HTTPCode=200_TotalTime=0.012201 2048 HTTPCode=200_TotalTime=0.005532 2048 HTTPCode=200_TotalTime=0.010107 2048 HTTPCode=200_TotalTime=0.012163 Welcome to nginx! HTTPCode=200_TotalTime=0.005452 Welcome to nginx! HTTPCode=200_TotalTime=0.009950 2048 HTTPCode=200_TotalTime=0.012082 Welcome to nginx! HTTPCode=200_TotalTime=0.005349 2048 HTTPCode=200_TotalTime=0.010142 2048 HTTPCode=200_TotalTime=0.012143 2048 HTTPCode=200_TotalTime=0.005507 ... HTTPCode=200_TotalTime=0.012149 Welcome to nginx! HTTPCode=200_TotalTime=0.005364 Welcome to nginx! HTTPCode=200_TotalTime=0.010021 Welcome to nginx! HTTPCode=200_TotalTime=0.012092 Welcome to nginx! HTTPCode=200_TotalTime=0.005463 Welcome to nginx! HTTPCode=200_TotalTime=0.010136 Welcome to nginx! This is the practice in case having AWS Load Balancer Controller for doing graceful deployment with RollingUpdate. But in summary, with the deployment strategy above, it is also recommended to design the client/backend application as stateless, implement retry and fault-tolerance. These mothod usually help to reduce the customer complain and provide better user experience for most common use case.\nConclusion Due to the current design of Kubernetes, it is involving the state inconsistent issue when you are exposing the service with Application Load Balancer. Therefore, in this article, I mentioned the potential issue when doing rolling update in the scenario having container service integrating with the AWS Load Balancer Controller (ALB Ingress Controller) and the way we can take to sove these issues.\n","wordCount":"2958","inLanguage":"en","datePublished":"2022-07-17T22:53:49+05:45","dateModified":"2022-07-17T22:53:49+05:45","author":{"@type":"Person","name":"Samrakshak karki"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://samrakshak.github.io/thenarrator.github.io/post/aws-load-balancer-controller-issue/"},"publisher":{"@type":"Organization","name":"The Narrator","logo":{"@type":"ImageObject","url":"https://samrakshak.github.io/thenarrator.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://samrakshak.github.io/thenarrator.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://samrakshak.github.io/thenarrator.github.io/images/logo2.png alt=logo aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://samrakshak.github.io/thenarrator.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://samrakshak.github.io/thenarrator.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://samrakshak.github.io/thenarrator.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://samrakshak.github.io/thenarrator.github.io/post/>Posts</a></div><h1 class=post-title>AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment)</h1><div class=post-meta><span title='2022-07-17 22:53:49 +0545 +0545'>July 17, 2022</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;2958 words&nbsp;·&nbsp;Samrakshak karki&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/post/aws-load-balancer-controller-issue.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h1 id=aws-load-balancer-controler-for-rolling-updates-zero-downtime-deployment>AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment)<a hidden class=anchor aria-hidden=true href=#aws-load-balancer-controler-for-rolling-updates-zero-downtime-deployment>#</a></h1><p>This article is describing the thing you need to aware when using ALB Ingress Controller (AWS Load Balancer Controller) to do deployment in your kubernetes appication and prevent 502 errors when there is rolling updates.</p><h1 id=what-is-aws-load-balancer-controler>What is AWS Load Balancer Controler<a hidden class=anchor aria-hidden=true href=#what-is-aws-load-balancer-controler>#</a></h1><p>Kubernetes doesn’t involve the Application Load Balancer (ALB) deployment in the native implementation for using Kubernetes service object with <code>type=LoadBalancer</code>. Therefore, if you need to expose your container service wih Appication Load Balancer (ALB) on EKS which should be the prefered way to expose your servies, it is recommneded to integrate with AWS Load Balancer Controller .
Guide for instalation: <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/deploy/installation/>AWS ALB Installaion</a></p><h3 id=how-does-it-work>How does it work<a hidden class=anchor aria-hidden=true href=#how-does-it-work>#</a></h3><p><img loading=lazy src=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.1/assets/images/controller-design.png alt=controller-design></p><p>source: <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.1/how-it-works/>aws-loadbalancer-controller</a></p><p>This section describes each step (circle) above. This example demonstrates satisfying 1 ingress resource.</p><p><strong>[1]</strong>: The controller watches for ingress events from the API server. When it finds ingress resources that satisfy its requirements, it begins the creation of AWS resources.</p><p><strong>[2]</strong>: An ALB (ELBv2) is created in AWS for the new ingress resource. This ALB can be internet-facing or internal. You can also specify the subnets it&rsquo;s created in using annotations.</p><p><strong>[3]</strong>: Target Groups are created in AWS for each unique Kubernetes service described in the ingress resource.</p><p><strong>[4]</strong>: Lilsteners are created for every port detailed in your ingress resource annotations. When no port is specified, sensible defaults (<code>80</code> or <code>443</code>) are used. Certificates may also be attached via annotations.</p><p><strong>[5]</strong>: Rules are created for each path specified in your ingress resource. This ensures traffic to a specific path is routed to the correct Kubernetes Service.</p><p>Along with the above, the controller also&mldr;</p><ul><li>deletes AWS components when ingress resources are removed from k8s.</li><li>modifies AWS components when ingress resources change in k8s.</li><li>assembles a list of existing ingress-related AWS components on start-up, allowing you to recover if the controller were to be restarted.</li></ul><p>Note: [AWS Load Balancer Controller](> <a href=https://github.com/kubernetes-sigs/aws-alb-ingress-controller>GitHub project - kubernetes-sigs/aws-alb-ingress-controller</a>)</p><h2 id=how-to-deploy-kubernetes-with-aws-load-balancer-controller>How to deploy Kubernetes with AWS Load Balancer Controller?<a hidden class=anchor aria-hidden=true href=#how-to-deploy-kubernetes-with-aws-load-balancer-controller>#</a></h2><p>Using Application Load Balancer as example, when running the controller, AWS Load Balancer Controller will be deployed as a Pod running on your worker node while continously monitor/watch your cluster state. Once there have any request for <code>Ingress</code> object creation, AWS Load Balancer Controller will help you to manage and create Application Load Balancer resource. <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/deploy/installation/>install aws-alb-controller</a></p><p>The deployment basically will run a copy of ALB Ingress Controller (pod/alb-ingress-controller-xxxxxxxx-xxxxx) in <code>kube-system</code>:</p><pre tabindex=0><code>NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   pod/alb-ingress-controller-5fd8d5d894-8kf7z   1/1     Running   0          28s

NAMESPACE     NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   deployment.apps/alb-ingress-controller   1/1     1            1           3m48s
</code></pre><p>In addition, the service can be deployed as <code>Ingress</code> Object. For example, if you tried to deploy the simple 2048 application:</p><pre tabindex=0><code>$ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-namespace.yaml
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-deployment.yaml
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-service.yaml
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/2048/2048-ingress.yaml
</code></pre><p>The file <code>2048-ingress.yaml</code> is mentioning the <code>annotations</code>, <code>spec</code> in format that supported by ALB Ingress Controller can recognize (Before Kubernetes 1.18):</p><pre tabindex=0><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: &#34;2048-ingress&#34;
  namespace: &#34;2048-game&#34;
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
  labels:
    app: 2048-ingress
spec:
  rules:
    - http:
        paths:
          - path: /*
            backend:
              serviceName: &#34;service-2048&#34;
              servicePort: 80
</code></pre><p>The ingress object will construct ELB Listeners according rules and forward the connection to the corresponding backend(<code>serviceName</code>), which match the group of service <code>service-2048</code>, any traffic match the rule <code>/*</code> will be routed to the group of selected Pods. In this case, Pods are exposed on the worker node based on <code>type=NodePort</code>:</p><p>Here is the definition of this Kubernetes service:</p><pre tabindex=0><code>apiVersion: v1
kind: Service
metadata:
  name: &#34;service-2048&#34;
  namespace: &#34;2048-game&#34;
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  type: NodePort
  selector:
    app: &#34;2048&#34;
</code></pre><h2 id=so--whats-the-problem>So … what’s the problem?<a hidden class=anchor aria-hidden=true href=#so--whats-the-problem>#</a></h2><p><strong>Zero downtime deployment</strong> is always a big challenge for DevOps/Operation team when running any kind of business. When you try to apply the controller as a solution to expose your service, it has a couple of things need to take care due to the behavior of Kubernetes, ALB and AWS Load Balancer Controller. To achieve zero downtime, you need to consider many perspectives, some new challenges will also popup when you would like to roll out the new deployment for your Pods with AWS Load Balancer Controller.</p><p>Let’s use the 2048 game as example to describe the scenario when you are trying to roll out a new version of your container application. In my environment, I have:</p><ul><li>A Kubernetes service <code>service/service-2048</code> using <code>NodePort</code> to expose the service</li><li>The deployment also have 5 copy of Pods for 2048 game, which is my backend application waiting for connections forwarding by Application Load Balancer (ALB)</li></ul><pre tabindex=0><code>NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
2048-game     pod/2048-deployment-58fb66554b-2f748          1/1     Running   0          53s
2048-game     pod/2048-deployment-58fb66554b-4hz5q          1/1     Running   0          53s
2048-game     pod/2048-deployment-58fb66554b-jdfps          1/1     Running   0          53s
2048-game     pod/2048-deployment-58fb66554b-rlpqm          1/1     Running   0          53s
2048-game     pod/2048-deployment-58fb66554b-s492n          1/1     Running   0          53s

NAMESPACE     NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
2048-game     service/service-2048   NodePort    10.100.53.119   &lt;none&gt;        80:30337/TCP    52s

NAMESPACE     NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
2048-game     deployment.apps/2048-deployment          5/5     5            5           53s
</code></pre><p>And for sure, once the controller correctly set up and provision the ELB resource, the full domain of ELB also will be recorded to the <code>Ingress</code> object:</p><pre tabindex=0><code>$ kubectl get ingress -n 2048-game
NAME           HOSTS   ADDRESS                                                                      PORTS   AGE
2048-ingress   *       xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com   80      11m
</code></pre><p>I can use the DNS name as endpoint to visit my container service:</p><pre tabindex=0><code>$ curl -s xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com | head
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;meta charset=&#34;utf-8&#34;&gt;
  &lt;title&gt;2048&lt;/title&gt;

  &lt;link href=&#34;style/main.css&#34; rel=&#34;stylesheet&#34; type=&#34;text/css&#34;&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;favicon.ico&#34;&gt;
  ...
</code></pre><p><img loading=lazy src=https://easoncao.com/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/alb-ingress-controller-with-2048-game-screenshot.png alt="2048 Game deployed with controller"></p><p>This application can be any kind of critical service you are running. As a SRE (Site Reliability Engineer), the goal and your duty is: we always try to ensure the service can run properly without any issue and no interruption. But, no one can one hundred percent guarantees the service can run properly if any changes applied, because system usually has its limitation and trade-off. Any service downtime can lead anyone of stakeholders(users, operation team or leadership) unhappy.</p><p>However, the question is that can we better to address these problem once we know the limitation and its behavior? In the following section I will try to walk through more realistic logic and phenomena by using the 2048 game as my sample service.</p><p>I am going to use a simple loop trick to continously access my service via the endpoint <code>[xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com](http://xxxxxxxx-2048game-xxxxxxxx-xxxx-xxxxxxxxx.ap-northeast-1.elb.amazonaws.com/)</code> to demonstrate a scenario: This is a popular web service and we always have customer need to access it, we basically have zero tolerance for any service downtime , as below:</p><pre tabindex=0><code>$ while true;do ./request-my-service.sh; sleep 0.1; done
HTTPCode=200_TotalTime=0.010038
HTTPCode=200_TotalTime=0.012131
HTTPCode=200_TotalTime=0.005366
HTTPCode=200_TotalTime=0.010119
HTTPCode=200_TotalTime=0.012066
HTTPCode=200_TotalTime=0.005451
HTTPCode=200_TotalTime=0.010006
HTTPCode=200_TotalTime=0.012084
HTTPCode=200_TotalTime=0.005598
HTTPCode=200_TotalTime=0.010086
HTTPCode=200_TotalTime=0.012162
HTTPCode=200_TotalTime=0.005278
HTTPCode=200_TotalTime=0.010326
HTTPCode=200_TotalTime=0.012193
HTTPCode=200_TotalTime=0.005347
...
</code></pre><p>Meanwhile, I am using <code>RollingUpdate</code> strategy in my Kubernetes deployment strategy with <code>maxUnavailable=25%</code>, which means, when Kubernetes need to update or patch(Like update the image or environment variables), the maximum number of unavailable Pods cannot exceed over <code>25%</code> as well as it ensures that at least 75% of the desired number of Pods are up (only replace 1-2 Pods if I have 5 copies at the same time):</p><pre tabindex=0><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: 2048-deployment
  namespace: 2048-game
spec:
  ...
  selector:
    matchLabels:
      app: &#34;2048&#34;
  ...
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
</code></pre><h3 id=scenario-rolling-the-new-container-image-to-existing-container-application-with-potential-service-downtime>Scenario: Rolling the new container image to existing container application with potential service downtime<a hidden class=anchor aria-hidden=true href=#scenario-rolling-the-new-container-image-to-existing-container-application-with-potential-service-downtime>#</a></h3><p>When rolling the new version of my container application (for example, I update my deployment by replacing the container image with the new image <code>nginx</code>), it potentially can have a period of time that can return <code>HTTP Status Code 502</code> error in my few hits.</p><p>If you are specifying the controller to use <code>instance</code> mode to register targets(Pods) to your ELB Target Group, it will use worker nodes’ instance ID and expose your service in that ELB target group with Kubernetes <code>NodePort</code>. In this case, the traffic will follow the Kubernetes networking design to do second tier of transmission according to <code>externalTrafficPolicy</code> defined in the Kubernetes <code>Service</code> object (No matter using <code>externalTrafficPolicy=Cluster</code> or <code>externalTrafficPolicy=Local</code>).</p><p>Due to the controller only care about to register Worker Node to the ELB target group, so if the scenario doesn’t involve the worker node replacement, the case basically have miniumun even no downtime(expect that it is rare to have downtime if the Kubernetes can perfectly handle the traffic forwarding); however, this is not how real world operate, few seconds downtime still can happen potentially due to the workflow below:</p><p><strong>This is the general workflow when the client reach out to the service endpoint (ELB) and how was traffic goes</strong></p><pre tabindex=0><code>Client ----&gt; ELB ----&gt; Worker Node (iptables) / In this step it might be forwarded to other Worker Node ----&gt; Pod
</code></pre><p>So, in these cases, you can see the downtime:</p><ul><li>(1) The client established the connection with ELB, ELB is trying to forward the request to the backend (the Worker Node), but the Worker Node is not ready to serve the Pod.</li><li>(2) Follow the iptables rules, the traffic be forwarded to the Pod just terminated due to RollingUpdate (Or the Pod just got the in-flight reqeust but immediately need to be terminated, the Pod flip to <code>Terminating</code> state. It haven’t response back yet, caused the ELB doesn’t get the response from Pod.)</li><li>(3) ELB established connection with the Worker Node-1, once the packet enter into the Worker Node-1, it follows the iptables then forward it to the Pod running on Worker Node-2 (jump out the current worker node), however, the Worker Node-2 just got terminated due to auto scaling strategy or any replacement due to upgrade, caused the connection lost.</li></ul><p>Let’s say if you try to remove the encapsulation layer of the Kubernetes networking design and make thing more easier based on the AWS supported CNI Plugin (Only rely on the ELB to forward the traffic to the Pod directly by using <code>IP mode</code> with annotation setting <code>[alb.ingress.kubernetes.io/target-type](http://alb.ingress.kubernetes.io/target-type): ip</code> in my Ingress object), you can see the downtime more obvious when Pod doing RollingUpdate. That’s because not only the problem we mentioned the issues in case (1)/(2)/(3), but also there has different topic on the behavior of the controller need to be covered if the question comes to <strong>zero downtime deployment</strong>:</p><p>Here is an example by using IP mode (<code>[alb.ingress.kubernetes.io/target-type](http://alb.ingress.kubernetes.io/target-type): ip</code>) as registration type to route traffic directly to the Pod IP</p><pre tabindex=0><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  namespace: game-2048
  name: ingress-2048
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  ingressClassName: alb
  rules:
    - http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: service-2048
              port:
                number: 80
</code></pre><p><img loading=lazy src=https://easoncao.com/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/alb-target-group-with-pod-in-ip-mode.png alt="An example when using IP mode in AWS Load BalancerController"></p><p>An example when using IP mode in AWS Load Balancer Controller - Can see my Pods all are registering with Pod owns IP address</p><p>Again follow the issue we mentioned (1) (2) (3), when doing the rolling update (I was replacing the image again in <code>IP mode</code>), similar problem can be observed. Potentially, you can have 10-15 seconds even longer downtime can be noticed if you are doing the same lab:</p><p><img loading=lazy src=https://easoncao.com/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/http-502-error-during-deployment-using-ip-mode.png alt="The HTTP 502 Error response from ELB (IP mode)"></p><p>The HTTP 502 Error response from ELB during the rolling update deployment (IP mode)</p><p>When Kubernetes is rolling the deployment, in the target group, you will see AWS Load Balancer Controller was issuing old targets draining process(Old Pods) in the meantime</p><p><img loading=lazy src=https://easoncao.com/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/old-target-doing-draining-in-target-group-with-ip-mode.png alt="Old targets were going to be draining state in target group"></p><p>Old targets were going to be draining state in target group</p><p>However, you still can see HTTP 502/504 errors exceed 3-10 seconds for a single requset</p><pre tabindex=0><code>HTTPCode=200_TotalTime=0.005413
2048
HTTPCode=200_TotalTime=0.009980
502 Bad Gateway
HTTPCode=502_TotalTime=3.076954
2048
HTTPCode=200_TotalTime=0.005700
2048
HTTPCode=200_TotalTime=0.010019
502 Bad Gateway
HTTPCode=502_TotalTime=3.081601
2048
HTTPCode=200_TotalTime=0.005527
502 Bad Gateway
HTTPCode=502_TotalTime=3.070947
502 Bad Gateway
HTTPCode=502_TotalTime=3.187812
504 Gateway Time-out
HTTPCode=504_TotalTime=10.006324
Welcome to nginx!
HTTPCode=200_TotalTime=0.011838
Welcome to nginx!
</code></pre><h3 id=the-issue-and-the-workflow-of-the-aws-load-balancer-controller>The issue and the workflow of the AWS Load Balancer Controller<a hidden class=anchor aria-hidden=true href=#the-issue-and-the-workflow-of-the-aws-load-balancer-controller>#</a></h3><p>Let’s use this scenario as it is a edge problem we need to consider for most use case. The issue generally is bringing out the core topic we want to address and giving a good entry point to dive deep into the workflow between the Kubernetes, AWS Load Balancer Controller and the ELB, which can lead HTTP status code 502/503(5xx) erros during deployment when having Pod termination.</p><p>Before diving into it, we need to know <strong>when a pod is being replaced, AWS Load Balancer Controller will register the new pod in the target group and removes the old Pods.</strong> However, at the same time:</p><ul><li>For the the new Pods, the target is in <code>initial</code> state, until it pass the defined health check threshold (ALB health check)</li><li>For the old Pods is remaining as <code>draining</code> state, until it completes draining action for the in-flight connection, or reaching out the <code>Deregistration delay</code> defined in the target group.</li></ul><p>Which result in the service to be unavailable and return HTTP 502.</p><p>To better understand that, I made the following diagrams. It might be helpful to you understanding the workflow:</p><h2 id=how-to-resolve-the-issue-and-meet-zero-downtime>How to resolve the issue and meet zero-downtime?<a hidden class=anchor aria-hidden=true href=#how-to-resolve-the-issue-and-meet-zero-downtime>#</a></h2><h3 id=factor-1-pod-readiness-gates>Factor-1: Pod Readiness Gates<a hidden class=anchor aria-hidden=true href=#factor-1-pod-readiness-gates>#</a></h3><p>Since version <a href=https://github.com/kubernetes-sigs/aws-alb-ingress-controller/releases/tag/v1.1.6>v1.1.6</a>, AWS Load Balancer Controller (ALB Ingress Controller) introduced <a href=https://github.com/kubernetes-sigs/aws-alb-ingress-controller/blob/master/docs/guide/ingress/pod-conditions.md>Pod readiness gates</a>. This feature can monitor the rolling deployment state and trigger the deployment pause due to any unexpected issue(such as: getting timeout error for AWS APIs), which guarantees you always have Pods in the Target Group even having issue on calling ELB APIs when doing rolling update.</p><h4 id=aws-load-balancer-controller-after-v2x>AWS Load Balancer Controller (After v2.x)<a hidden class=anchor aria-hidden=true href=#aws-load-balancer-controller-after-v2x>#</a></h4><p><strong>For now, if you are using controller later than v2, the readiness gate configuration can be automatically injected to the pod spec</strong> by defining the label <code>[elbv2.k8s.aws/pod-readiness-gate-inject](http://elbv2.k8s.aws/pod-readiness-gate-inject): enabled</code> to your Kubernetes namespace.</p><pre tabindex=0><code>$ kubectl create namespace readiness
namespace/readiness created

$ kubectl label namespace readiness elbv2.k8s.aws/pod-readiness-gate-inject=enabled
namespace/readiness labeled

$ kubectl describe namespace readiness
Name:         readiness
Labels:       elbv2.k8s.aws/pod-readiness-gate-inject=enabled
Annotations:  &lt;none&gt;
Status:       Active
</code></pre><p>So defining legacy fields <code>readinessGates</code> and <code>conditionType</code> are not required if you are using controller later than <code>v2.0</code>. <strong>If you have a pod spec with legacy readiness gate configuration, ensure you label the namespace and create the Service/Ingress objects before applying the pod/deployment manifest. The controller will remove all legacy readiness-gate configuration and add new ones during pod creation.</strong></p><h3 id=factor-2-graceful-shutdown-your-applications>Factor-2: Graceful shutdown your applications<a hidden class=anchor aria-hidden=true href=#factor-2-graceful-shutdown-your-applications>#</a></h3><p>For existing connections(As mentioned in the workflow-4), the case is involving the gracefully shutdown/termination handling in Kubernetes. Therefore, it is requires to use the method provided by Kubernetes.</p><p>You can use <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>Pod Lifecycle</a> with <code>preStop</code> hook and make some pause(like using <code>sleep</code> command) for Pod termination. This trick ensures ALB can have some time to completely remove old targets on Target Group (It is recommended to adjust longer based on your <code>Deregistration delay</code>):</p><pre tabindex=0><code>    lifecycle:
      preStop:
        exec:
          command: [&#34;/bin/sh&#34;, &#34;-c&#34;, &#34;sleep 40&#34;]
  terminationGracePeriodSeconds: 70
</code></pre><blockquote><p>Note: If a container has a preStop hook configured, that runs before the container enters the Terminated state. Also, if the preStop hook needs longer to complete than the default grace period allows, you must modify <code>terminationGracePeriodSeconds</code> to suit this.</p></blockquote><h3 id=an-example-to-achieve-zero-downtime-when-doing-rolling-update-after-applying-methods-above>An example to achieve zero downtime when doing rolling update after applying methods above<a hidden class=anchor aria-hidden=true href=#an-example-to-achieve-zero-downtime-when-doing-rolling-update-after-applying-methods-above>#</a></h3><p>First apply the label to the namespace so the controller can automatically inject the readiness gate:</p><pre tabindex=0><code>apiVersion: v1
kind: Namespace
metadata:
  name: 2048-game
  labels:
    elbv2.k8s.aws/pod-readiness-gate-inject: enabled
</code></pre><pre tabindex=0><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: &#34;2048-deployment&#34;
  namespace: &#34;2048-game&#34;
spec:
  selector:
    matchLabels:
      app: &#34;2048&#34;
  replicas: 5
  template:
    metadata:
      labels:
        app: &#34;2048&#34;
    spec:

      # This would be optional if you are using controller after v2.x
      readinessGates:
      - conditionType: target-health.alb.ingress.k8s.aws/2048-ingress_service-2048_80

      terminationGracePeriodSeconds: 70
      containers:
      - image: alexwhen/docker-2048
        imagePullPolicy: Always
        name: &#34;2048&#34;
        ports:
        - containerPort: 80
        lifecycle:
          preStop:
            exec:
              command: [&#34;/bin/sh&#34;, &#34;-c&#34;, &#34;sleep 40&#34;]
</code></pre><p>Here is an example after following the practice I was getting a try. The deployment will apply the feature and can see the status of the readiness gates:</p><pre tabindex=0><code>$ kubectl get pods -n 2048-game -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP               NODE                                              NOMINATED NODE   READINESS GATES
2048-deployment-99b6fb474-c97ht   1/1     Running   0          78s   192.168.14.209   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1
2048-deployment-99b6fb474-dcxfs   1/1     Running   0          78s   192.168.31.47    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1
2048-deployment-99b6fb474-kvhhh   1/1     Running   0          54s   192.168.29.6     XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1
2048-deployment-99b6fb474-vhjbg   1/1     Running   0          54s   192.168.18.161   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1
2048-deployment-99b6fb474-xfd5q   1/1     Running   0          78s   192.168.16.183   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.compute.internal   &lt;none&gt;           1/1
</code></pre><p>Once rolling the new version of the container image, the deployment goes smoothly and prevent the downtime issue as mentioned in previous paragraphs:</p><p><img loading=lazy src=https://easoncao.com/assets/images/posts/2020/10/zero-downtime-deployment-when-using-alb-ingress-controller/zero-downtime-deployment-with-alb-demo.png alt="Zero downtime with AWS Load Balancer Controller - Can see the targets are gracefully replaced when the Kubernetes is doing rolling update"></p><p>Zero downtime with AWS Load Balancer Controller - Can see the targets are gracefully replaced when the Kubernetes is doing rolling update</p><p>In my scenario, the Kubernetes need to take at least 40 seconds termination period for single Pod, so the old targets are gradually moved out instead of remove all of them at once within few seconds, until entire target group only exists new targets.</p><p>Therefore, you probably also need to notice the <code>Deregistration delay</code> defined in your ELB Target Group, which can be updated through <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/ingress/annotations/#target-group-attributes>the annotation</a>:</p><pre tabindex=0><code>alb.ingress.kubernetes.io/target-group-attributes: deregistration_delay.timeout_seconds=30
</code></pre><p>In this case, it is recommended to be less than 40 seconds so ELB can drain your old targets before the Pod completely shutdown.</p><p>With the configuration, client can get normal responses from old Pods/existing connection during the deployment:</p><pre tabindex=0><code>HTTPCode=200_TotalTime=0.012028
2048
HTTPCode=200_TotalTime=0.005383
2048
HTTPCode=200_TotalTime=0.010174
2048
HTTPCode=200_TotalTime=0.012233
Welcome to nginx!
HTTPCode=200_TotalTime=0.007116
2048
HTTPCode=200_TotalTime=0.010090
2048
HTTPCode=200_TotalTime=0.012201
2048
HTTPCode=200_TotalTime=0.005532
2048
HTTPCode=200_TotalTime=0.010107
2048
HTTPCode=200_TotalTime=0.012163
Welcome to nginx!
HTTPCode=200_TotalTime=0.005452
Welcome to nginx!
HTTPCode=200_TotalTime=0.009950
2048
HTTPCode=200_TotalTime=0.012082
Welcome to nginx!
HTTPCode=200_TotalTime=0.005349
2048
HTTPCode=200_TotalTime=0.010142
2048
HTTPCode=200_TotalTime=0.012143
2048
HTTPCode=200_TotalTime=0.005507

...
HTTPCode=200_TotalTime=0.012149
Welcome to nginx!
HTTPCode=200_TotalTime=0.005364
Welcome to nginx!
HTTPCode=200_TotalTime=0.010021
Welcome to nginx!
HTTPCode=200_TotalTime=0.012092
Welcome to nginx!
HTTPCode=200_TotalTime=0.005463
Welcome to nginx!
HTTPCode=200_TotalTime=0.010136
Welcome to nginx!
</code></pre><p>This is the practice in case having AWS Load Balancer Controller for doing graceful deployment with <code>RollingUpdate</code>. But in summary, with the deployment strategy above, it is also recommended to design the client/backend application as stateless, implement retry and fault-tolerance. These mothod usually help to reduce the customer complain and provide better user experience for most common use case.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Due to the current design of Kubernetes, it is involving the state inconsistent issue when you are exposing the service with Application Load Balancer. Therefore, in this article, I mentioned the potential issue when doing rolling update in the scenario having container service integrating with the AWS Load Balancer Controller (ALB Ingress Controller) and the way we can take to sove these issues.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://samrakshak.github.io/thenarrator.github.io/post/introducion/><span class=title>Next »</span><br><span>Introducion</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment) on twitter" href="https://twitter.com/intent/tweet/?text=AWS%20Load%20Balancer%20Controler%20for%20Rolling%20Updates%20%28Zero%20downtime%20deployment%29&url=https%3a%2f%2fsamrakshak.github.io%2fthenarrator.github.io%2fpost%2faws-load-balancer-controller-issue%2f&hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fsamrakshak.github.io%2fthenarrator.github.io%2fpost%2faws-load-balancer-controller-issue%2f&title=AWS%20Load%20Balancer%20Controler%20for%20Rolling%20Updates%20%28Zero%20downtime%20deployment%29&summary=AWS%20Load%20Balancer%20Controler%20for%20Rolling%20Updates%20%28Zero%20downtime%20deployment%29&source=https%3a%2f%2fsamrakshak.github.io%2fthenarrator.github.io%2fpost%2faws-load-balancer-controller-issue%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsamrakshak.github.io%2fthenarrator.github.io%2fpost%2faws-load-balancer-controller-issue%2f&title=AWS%20Load%20Balancer%20Controler%20for%20Rolling%20Updates%20%28Zero%20downtime%20deployment%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsamrakshak.github.io%2fthenarrator.github.io%2fpost%2faws-load-balancer-controller-issue%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment) on whatsapp" href="https://api.whatsapp.com/send?text=AWS%20Load%20Balancer%20Controler%20for%20Rolling%20Updates%20%28Zero%20downtime%20deployment%29%20-%20https%3a%2f%2fsamrakshak.github.io%2fthenarrator.github.io%2fpost%2faws-load-balancer-controller-issue%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share AWS Load Balancer Controler for Rolling Updates (Zero downtime deployment) on telegram" href="https://telegram.me/share/url?text=AWS%20Load%20Balancer%20Controler%20for%20Rolling%20Updates%20%28Zero%20downtime%20deployment%29&url=https%3a%2f%2fsamrakshak.github.io%2fthenarrator.github.io%2fpost%2faws-load-balancer-controller-issue%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>© Samrakshak Karki 2022</span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>